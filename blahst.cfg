#X11 or Wayland (2nd line may catch edge cases):
wm="$XDG_SESSION_TYPE"
#wm="${XDG_SESSION_TYPE:-$(loginctl show-session $(loginctl | grep $(whoami) | awk '{print $1}') -p Type --value)}"

#---------start-section-common

# Hardcoded temp wav file to store the speech audio and get overwritten every time (in RAM):
ramf="/dev/shm/wfile-$USER"
# KEEP_TEMP_FILE=0
#Set the number of processing threads for whisper.cpp inference (adjust for your case):
NTHR=8
#It seems that the optimum number of transcribe threads should equal half CPU processing cores:
#NTHR=$(( $(getconf _NPROCESSORS_ONLN) / 2 ))
# Use clipboard to paste from unless PRIMESEL set to 1 to use PRIMARY selection (middle mouse button to paste): 
PRIMESEL=0
# Try to paste automatically on completed transcription, reqires xdotool and such:
AUTOPASTE=0
# Set the next to 1 to default to chat (not sigle shot) mode with the AI assistant of wsiAI:
CHATMODE=0
#Provide hardcoded whisper.cpp hostname and port. To be used when invoked with -n option from CLI:
#(Can be overwritten when command line argument IP:PORT is supplied instead of '-n')
WHOST="127.0.0.1"
WPORT="58080"             
#The superdirectory where AI models (Whisper, LLMS, TTS etc.) are stored:
AI="$HOME/AI/Models"
# Default whisper.cpp model file for local ASR inference (base.en provides excellent WER for English only)
# Store temp files in memory for speed and to reduce SSD/HDD "grinding"
#WMODEL="/dev/shm/ggml-base.en.bin"
WMODEL=${WHISPER_DMODEL:-"$AI/whisper/ggml-base.en.bin"}
# Uncomment to use available whisperfile (WITH BUILT-IN MODEL) instead of standalone whisper.cpp or whisper.cpp server
#WHISPERFILE="whisper-tiny.en.llamafile" #available from https://huggingface.co/Mozilla/whisperfile/tree/main
#--------end-section----wsi--wsiml--blooper

#Provide hardcoded llama.cpp hostname and port.
LHOST="127.0.0.1"
LPORT="58090"             
#Use the following large language model with llama.cpp for the interactive assistant/translator etc.:
LLMODEL="$AI/gemma-3-4b-it-Q6_K_L.gguf" #avalable from https://huggingface.co/bartowski/gemma3-4b-it-GGUF/tree/main   
#Use the following LIGHTWEIGHT large language model with llama.cpp for a simpler interactive assistant:
LIGHTLMODEL="$AI/gemma-3-1b-it-Q6_K_L.gguf" # available from https://huggingface.co/google/gemma3-1b-it-GGUF
#LIGHTLMODEL="$AI/granite-3.0-2b-instruct-Q6_K_L.gguf"
#More potent model with larger VRAM footprint:
HEAVYMODEL="$AI/gemma-3-27b-it-IQ3_XXS.gguf"
#For coding tasks, invoked with "Programmer...":
CODEMODEL="$AI/Qwen2.5-Coder-14B-Instruct-Q5_K_L.gguf"
#Default llama.cpp executable:
llamf="llam" #This is an existing systemwide symbolic link to llama-cli from llama.cpp
#Uncomment the following to use a LLM from a llammafile instead of standalone llamma.cpp:
#LLAMAFILE="$AI/gemma-2-2b-it.Q6_K.llamafile"
#LLAMAFILE="llamafile-0.8.16" #available from https://github.com/Mozilla-Ocho/llamafile/releases/tag/0.8.16
#LLAMAFILE="llamafile" # on my system a symbolic link to the current version of llamafile
#--------end-section----wsiAI

#--------Extras for BLAHSTBOT----------
#Use the following heavy hitter for tasks that require more "inteligence" (slower):
BOTMODEL="$AI/gemma-3-12b-it-q4_0_s.gguf" # available from https://huggingface.co/bartowski/gemma3-12b-it-GGUF
#BOTMODEL="$AI/gemma-3-27b-it-IQ3_XXS.gguf"
#BOTMODEL="$AI/Qwen3-14B-Q5_K_L.gguf
#The next is recommended for the Personal Doctor role, assigned automatically in the select_role() function below.
#BOTMODEL="$AI/medgemma-27b-text-it-UD-IQ3_XXS.gguf"
#Choose these carefully and optimize for your system hardware. RTRFM @ `llama-server --help`
BMOPTIONS="-ngl 99 -fa -c 16384 -nkvo -ctk q8_0 -ctv q8_0 --min-p 0.01 --top-k 64 --top-p 0.95 --no-webui --log-disable --no-mmap --mlock"

#System prompts for the chatbot model of choice, add yours here (key, value)):
typeset -A BMPROMPT=(
["Personal Doctor"]='You are MedGemma, a helpful advisor on all things medical.'
["Science  Assistant"]='You are a helpful science assistant who answers briefly and precisely. You use logic and science-based reasoning to provide statements, governed by Physics principles. When you do not have sufficient information, please, say so.'
["Multilingual"]='You are a helpful assistant who answers briefly and precisely. Ocasionally, I may ask you to speak a specific language. Then, for any language different from English, you will enclose the specific language response in tags with capitalized ISO 639-1 codes, for example <FR>Je comprends!</FR>, <ES>Una historia de amor</ES>. etc. Do not enclose transliterations such as pinyin and do not enclose english explanations, only the non-English language elements.'
["Language Teacher"]='You are a language teacher. When I ask you to help me with a specific language, you will translate my statements in the desired language, repeating the foreign language statements so I can hear them clearly. Any text that is not in English must be in enclosed in tags with capitalized ISO 639-1 codes, for example <FR>Je comprends!</FR>, <ES>Una historia de amor</ES>, <ZH>茉莉花茶</ZH>, etc. Again, enclose only the non-English text and all the non-English text. Where needed, provide disection of foreign language elements supported by clear English explanations. For example: <ZH>乙</ZH> (yǐ) in <ZH>艺</ZH> (yì - art/skill).'
["Language Interpreter"]='You are a language interpreter. When I ask you to interpret to a specific language, you will translate my statements in the desired language, repeating the foreign language statements so I can hear them clearly. Any text that is not in English must be in enclosed in tags with capitalized ISO 639-1 codes, for example <FR>Je comprends!</FR>, <ES>Una historia de amor</ES>, <ZH>茉莉花茶</ZH>, etc. Again, enclose only the non-English text and all the non-English text.'
["Chinese Translator"]='You are a highly accurate and reliable English-to-Simplified Chinese translator. Your sole purpose is to translate English text into Simplified Chinese, ensuring the translation is as accurate as possible within the given context. Provide only the Chinese translation enclosed in tags with capitalized ISO 639-1 codes, for example: <ZH>茉莉花茶</ZH>. Do not include any explanations, commentary, or additional text.'
)

#Initial LLM greeting. Modify as desired:
BOTGREETING="I am ready. You can simply speak to me or select any text with the mouse to append context to your verbal questions. What shall we chat about?"
#Use the following text-to-speech Piper model for human-like audio response in English:
TTSMODEL="$AI/piper/en_US-lessac-low.onnx"  #available from https://github.com/rhasspy/piper/blob/master/VOICES.md
#The above TTS model has sample rate 16000 (if you change the model, adjust the sample rate below):
rtts="16000" 
#Use the following text-to-speech Piper model for human-like audio response in the language (e.g. chinese) of the LLM translator function:
TRANSMODEL="$AI/piper/zh_CN-huayan-medium.onnx"  #available from https://github.com/rhasspy/piper/blob/master/VOICES.md
#The above model has sample rate 22050 (if you change it, adjust the sample rate below):
rtrans="22050"

#Local piper language and model configuration for multilingual speech chat:
PPRMDIR=$AI/piper
#When adding new language models use only ISO 639-1 compliant language codes as keys:
typeset -A TTSM=(
["ZH"]="zh_CN-huayan-medium.onnx"
["JO"]="ar_JO-kareem-medium.onnx"
["DA"]="da_DK-talesyntese-medium.onnx"
["DE"]="de_DE-kerstin-low.onnx"
["EL"]="el_GR-rapunzelina-low.onnx"
["GB"]="en_GB-alan-low.onnx"
["US"]="en_US-joe-medium.onnx"
["EN"]="en_US-lessac-low.onnx"
["ES"]="es_ES-mls_9972-low.onnx"
["FI"]="fi_FI-harri-medium.onnx"
["FR"]="fr_FR-gilles-low.onnx"
["IT"]="it_IT-paola-medium.onnx"
["NL"]="nl_NL-mls-medium.onnx"
["PT"]="pt_PT-tugão-medium.onnx"
["RO"]="ro_RO-mihai-medium.onnx"
["RU"]="ru_RU-ruslan-medium.onnx"
["SV"]="sv_SE-nst-medium.onnx"
["TR"]="tr_TR-dfki-medium.onnx"
["UA"]="uk_UA-ukrainian_tts-medium.onnx"
)

typeset -A modrate=(
["ZH"]=22050
["JO"]=22050
["DA"]=22050
["DE"]=16000
["EL"]=16000
["GB"]=16000
["US"]=22050
["EN"]=16000
["ES"]=16000
["FI"]=22050
["FR"]=16000
["IT"]=22050
["NL"]=22050
["PT"]=22050
["RO"]=22050
["RU"]=22050
["SV"]=22050
["TR"]=22050
["UA"]=22050
)
