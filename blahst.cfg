#X11 or Wayland (2nd line may catch edge cases):
wm="$XDG_SESSION_TYPE"
#wm="${XDG_SESSION_TYPE:-$(loginctl show-session $(loginctl | grep $(whoami) | awk '{print $1}') -p Type --value)}"

#---USER CONFIGURATION BLOCK----------------------------------------------------------------------
#Please, adjust the variables here to suit your environment:
# Store temp files in memory for speed and to reduce SSD/HDD "grinding":
TEMPD='/dev/shm'
# Hardcoded temp wav file to store the speech audio and get overwritten every time (in RAM):
ramf="$TEMPD/wfile"
#deleteramf=0
#Set the number of processing threads for whisper.cpp inference (adjust for your case):
NTHR=8
#It seems that the optimum number of transcribe threads should equal half CPU processing cores:
#NTHR=$(( $(getconf _NPROCESSORS_ONLN) / 2 ))
# Use clipboard to paste from unless PRIMESEL set to 1 to use PRIMARY selection (middle mouse button to paste): 
PRIMESEL=0
# Try to paste automatically on completed transcription, reqires xdotool and such:
AUTOPASTE=1
# Set the next to 1 to default to chat (not sigle shot) mode with the AI assistant of wsiAI:
CHATMODE=0
#Provide hardcoded whisper.cpp hostname and port. To be used when invoked with -n option from CLI:
#(Can be overwritten when command line argument IP:PORT is supplied instead of '-n')
WHOST="127.0.0.1"
WPORT="58080"             
#Provide hardcoded llama.cpp hostname and port.
LHOST="127.0.0.1"
LPORT="58090"             
#The superdirectory where AI models (Whisper, LLMS, TTS etc.) are stored:
AI="$HOME/AI/Models"
# Default whisper.cpp model file for local ASR inference (base.en provides excellent WER for English only)
#WMODEL="$TEMPD/ggml-base.en.bin"
WMODEL=${WHISPER_DMODEL:-"$AI/whisper/ggml-base.en.bin"}
# Uncomment to use available whisperfile (WITH BUILT-IN MODEL) instead of standalone whisper.cpp or whisper.cpp server
#WHISPERFILE="whisper-tiny.en.llamafile" #available from https://huggingface.co/Mozilla/whisperfile/tree/main
#Use the following large language model with llama.cpp for the interactive assistant/translator etc.:
LLMODEL="$AI/gemma-3-4b-it-Q6_K_L.gguf" #avalable from https://huggingface.co/bartowski/gemma3-4b-it-GGUF/tree/main   
#Use the following LIGHTWEIGHT large language model with llama.cpp for a simpler interactive assistant:
LIGHTLMODEL="$AI/gemma-3-1b-it-Q6_K_L.gguf" # available from https://huggingface.co/google/gemma3-1b-it-GGUF
#LIGHTLMODEL="$AI/granite-3.0-2b-instruct-Q6_K_L.gguf"
#More potent model with larger VRAM footprint:
HEAVYMODEL="$AI/gemma-3-27b-it-IQ3_XXS.gguf"
#For coding tasks, invoked with "Programmer...":
CODEMODEL="$AI/Qwen2.5-Coder-14B-Instruct-Q5_K_L.gguf"
#Default llama.cpp executable:
llamf="llam" #This is an existing systemwide symbolic link to llama-cli from llama.cpp
#Uncomment the following to use a LLM from a llammafile instead of standalone llamma.cpp:
#LLAMAFILE="$AI/gemma-2-2b-it.Q6_K.llamafile"
#LLAMAFILE="llamafile-0.8.16" #available from https://github.com/Mozilla-Ocho/llamafile/releases/tag/0.8.16
#LLAMAFILE="llamafile" # on my system a symbolic link to the current version of llamafile
#--------BLAHSTBOT SECTION----------
#Use the following heavy hitter for tasks that require more "inteligence" (slower):
BOTMODEL="$AI/gemma-3-12b-it-q4_0_s.gguf" # available from https://huggingface.co/bartowski/gemma3-12b-it-GGUF
#BOTMODEL="$AI/gemma-3-27b-it-IQ3_XXS.gguf"
#BOTMODEL="$AI/Qwen3-14B-Q5_K_L.gguf
#Choose these carefully and optimize for your system hardware. RTRFM @ `llama-server --help`
BMOPTIONS="-ngl 99 -fa -c 16384 -nkvo -ctk q8_0 -ctv q8_0 --min-p 0.01 --top-k 64 --top-p 0.95 --no-webui --log-disable --no-mmap --mlock"

#System prompts for the chatbot model of choice, add yours here (key, value)):
typeset -A BMPROMPT=(
["Store Bot"]='You are a helpful store assistant who helps customers with the items on sale. You answer questions and make suggestions based on a provided table of products on sale.'
["Science  Assistant"]='You are a helpful science assistant who answers briefly and precisely. You use logic and science-based reasoning to provide statements, governed by Physics principles. When you do not have sufficient information, please, say so.'
["Multilingual"]='You are a helpful assistant who answers briefly and precisely. Ocasionally, I may ask you to speak a specific language. Then, for any language different from English, you will enclose the specific language response in tags with capitalized ISO 639-1 codes, for example <FR>Je comprends!</FR>, <ES>Una historia de amor</ES>. etc. Do not enclose transliterations such as pinyin and do not enclose english explanations, only the non-English language elements.'
["Language Teacher"]='You are a language teacher. When I ask you to help me with a specific language, you will translate my statements in the desired language, repeating the foreign language statements so I can hear them clearly. Any text that is not in English must be in enclosed in tags with capitalized ISO 639-1 codes, for example <FR>Je comprends!</FR>, <ES>Una historia de amor</ES>, <ZH>茉莉花茶</ZH>, etc. Again, enclose only the non-English text and all the non-English text. Where needed, provide disection of foreign language elements supported by clear English explanations. For example: <ZH>乙</ZH> (yǐ) in <ZH>艺</ZH> (yì - art/skill).'
["Language Interpreter"]='You are a language interpreter. When I ask you to interpret to a specific language, you will translate my statements in the desired language, repeating the foreign language statements so I can hear them clearly. Any text that is not in English must be in enclosed in tags with capitalized ISO 639-1 codes, for example <FR>Je comprends!</FR>, <ES>Una historia de amor</ES>, <ZH>茉莉花茶</ZH>, etc. Again, enclose only the non-English text and all the non-English text.'
["Chinese Translator"]='You are a highly accurate and reliable English-to-Simplified Chinese translator. Your sole purpose is to translate English text into Simplified Chinese, ensuring the translation is as accurate as possible within the given context. Provide only the Chinese translation enclosed in tags with capitalized ISO 639-1 codes, for example: <ZH>茉莉花茶</ZH>. Do not include any explanations, commentary, or additional text.'
)

#Initial LLM greeting. Modify as desired:
BOTGREETING="I am ready. You can simply speak to me or select any text with the mouse to append context to your verbal questions. What shall we chat about?"
#Use the following text-to-speech Piper model for human-like audio response in English:
TTSMODEL="$AI/piper/en_US-lessac-low.onnx"  #available from https://github.com/rhasspy/piper/blob/master/VOICES.md
#The above TTS model has sample rate 16000 (if you change the model, adjust the sample rate below):
rtts="16000" 
#Use the following text-to-speech Piper model for human-like audio response in the language (e.g. chinese) of the LLM translator function:
TRANSMODEL="$AI/piper/zh_CN-huayan-medium.onnx"  #available from https://github.com/rhasspy/piper/blob/master/VOICES.md
#The above model has sample rate 22050 (if you change it, adjust the sample rate below):
rtrans="22050"

#Local piper language and model configuration for multilingual speech chat:
PPRMDIR=$AI/piper
#When adding new language models use only ISO 639-1 compliant language codes as keys:
typeset -A TTSM=(
["ZH"]="zh_CN-huayan-medium.onnx"
["JO"]="ar_JO-kareem-medium.onnx"
["DA"]="da_DK-talesyntese-medium.onnx"
["DE"]="de_DE-kerstin-low.onnx"
["EL"]="el_GR-rapunzelina-low.onnx"
["GB"]="en_GB-alan-low.onnx"
["US"]="en_US-joe-medium.onnx"
["EN"]="en_US-lessac-low.onnx"
["ES"]="es_ES-mls_9972-low.onnx"
["FI"]="fi_FI-harri-medium.onnx"
["FR"]="fr_FR-gilles-low.onnx"
["IT"]="it_IT-paola-medium.onnx"
["NL"]="nl_NL-mls-medium.onnx"
["PT"]="pt_PT-tugão-medium.onnx"
["RO"]="ro_RO-mihai-medium.onnx"
["RU"]="ru_RU-ruslan-medium.onnx"
["SV"]="sv_SE-nst-medium.onnx"
["TR"]="tr_TR-dfki-medium.onnx"
["UA"]="uk_UA-ukrainian_tts-medium.onnx"
)

typeset -A modrate=(
["ZH"]=22050
["JO"]=22050
["DA"]=22050
["DE"]=16000
["EL"]=16000
["GB"]=16000
["US"]=22050
["EN"]=16000
["ES"]=16000
["FI"]=22050
["FR"]=16000
["IT"]=22050
["NL"]=22050
["PT"]=22050
["RO"]=22050
["RU"]=22050
["SV"]=22050
["TR"]=22050
["UA"]=22050
)

#Set next to 1 ONLY if the program has confirmed that all dependenncies are met and you do not want to check in the future:
blahst_deps=0
#Can be done automatically by user input during initial runof blahst_depends(). 
#---END USER CONFIG BLOCK------------------------------------------------------------------------

#---HELPER FUNCTIONS--------------------------------------------------------------------------
#Notification code, prefers zenity, then notify-send, which should be available across the
#most distributions, in package libnotify or libnotify-bin:
desknote() {
    local title="$1"
    local message="$2"

    if command -v zenity &> /dev/null; then
        zenity --info --text="${title}.\n$message"
    elif command -v notify-send &> /dev/null; then
        notify-send "$title" "$message"
    elif command -v kdialog &> /dev/null; then
        kdialog --passivepopup "$message" 5
    else
        echo "Notification Message: $message" >&2
        echo "Please install zenity or notify-send to use notifications." >&2
        echo "You can install either using your package manager, e.g.:" >&2
        echo "  sudo apt-get install zenity or sudo apt-get install libnotify-bin" >&2
        echo "  sudo yum install zenity or sudo apt-get install libnotify" >&2
        echo "  sudo pacman -S zenity, etc." >&2
    fi
}
#Choose a prompt (role) for the LLM:
select_role () {
    local keys=("${(@k)BMPROMPT}")  
    local role=$(zenity --list --text="Select an LLM role:" --column="Role" "${keys[@]}")
    echo $BMPROMPT[$role] > $TEMPD/chatbot_prompt
}

#Checks for dependencies:
blahst_depends() {
    #---CHECK DEPENDENCIES.-----------------------------------
    #The install-wsi script should have taken care of this, left here for the manual install.
    command -v curl &>/dev/null || { echo "curl is required. Please, install curl" >&2 ; exit 1 ; }
    #The next is needed if curl requests json output from the whisper.cpp or llama.cpp server;
    command -v jq &>/dev/null || { echo "jq is required. Please, install jq" >&2 ; exit 1 ; }
    command -v sox &>/dev/null || { echo "sox is required. Please, install sox" >&2 ; exit 1 ; }
    [[ -n $WHISPERFILE ]] || command -v transcribe &>/dev/null || { echo -e "Please, install whisper.cpp (see https://github.com/ggerganov/whisper.cpp)\
    \nor download a whisperfile portable executable w model inside (see https://huggingface.co/Mozilla/whisperfile/tree/main)\
    \nand create 'transcribe' in your PATH as a symbolic link to the chosen executable, e.g.\n \
     'ln -s /full/path/to/whisper.cpp/main \$HOME/.local/bin/transcribe'" >&2 ; exit 1 ; }
    command -v llam &>/dev/null || { echo -e "For interaction with an AI, install llama.cpp (see https://github.com/ggerganov/llama.cpp)\
    \nor download a llamafile portable executable (see https://huggingface.co/Mozilla)\
    \nand create in your PATH a symbolic link to the llama-cli executable, e.g.\n \
     'ln -s /full/path/to/llama.cpp/llama-cli \$HOME/.local/bin/llam'" >&2 ; exit 1 ; }
    command -v piper &>/dev/null || { echo -e "Please, install piper text-to-speech (see https://github.com/rhasspy/piper)\
    \nto use the AI assistant and translator features with human-like voice response." >&2 ; exit 1 ; }
    # We will use zenity or notify-send (part of libnotify or libnotify-bin in some distros) for desktop notifications:
    command -v zenity &>/dev/null || command -v notify-send &>/dev/null || { echo "zenity or notify-send needed for error reporting. Please, install zenity or libnotify-bin" >&2 ; exit 1 ; }
    #Now let's check if we are in X11 or Wayland and use the right utility:
    if [[ wm == "wayland" ]]; then
        command -v wl-copy &>/dev/null || { echo "wl-copy is needed for the clipboard. Please, install wl-copy" >&2 ; exit 1 ; } 
        if (( $AUTOPASTE )); then 
           command -v ydotool &>/dev/null || { echo "To have the transcribed text pasted automatically at the cursor position, please install ydotool" >&2 ; exit 1 ; }
        fi
    elif [[ wm == "x11" ]]; then
        command -v xsel &>/dev/null || { echo "We rely on xsel for the clipboard. Please, install xsel." >&2 ; exit 1 ; }
        if (( $AUTOPASTE )); then 
           command -v xdotool &>/dev/null || { echo "To have the transcribed text pasted automatically at the cursor position, please install xdotool" >&2 ; exit 1 ; }
        fi
    fi
    for llv in $WMODEL $LLMODEL $LIGHTLMODEL $HEAVYMODEL $TTSMODEL $TRANSMODEL $CODEMODEL $BOTMODEL
    do
    [[ -f $llv ]] || { desknote "Model file not found: " "$llv was not found. \nPlease, adjust in USER CONFIGURATION BLOCK." ; exi1 1 ; }
    done 
    [[ -n $LLAMAFILE ]] && { command -v $LLAMAFILE &>/dev/null || { desknote "Not found: " "Executable $LLAMAFILE not found." ; exit 1 ; } }
    [[ -n $WHISPERFILE ]] && { command -v $WHISPERFILE &>/dev/null || { desknote "Not found: " "Executable $WHISPERFILE not found." ; exit 1 ; } }
    #All PASS...
    command -v zenity &>/dev/null && {
    if zenity --question --text="Dependencies of BlahST are met. Do you want to skip future dependency checks?"; then
        # Update the configuration file to set blahst_deps to 1 (we are sourcing this so hardcoding the filename)
        sed -i 's/^blahst_deps=0/blahst_deps=1/' $HOME/.local/bin/blahst.cfg
        desknote "Skipping future dependency checks confirmed!" "To undo, manually set blahst_deps=0 in blahst.cfg."
    fi
    } || {
    echo -e "\nLooks like you have all dependencies installed.\nTo disable future checks, must set blahst_deps=1 in blahst.cfg.\n"
    read -p "Do you want me to set 'blahst_deps=1' and skip future dependency checks? (y/n) " answer
    if [[ "$answer" == "y" || "$answer" == "Y" ]]; then
        # Update the configuration file to set blahst_deps=1
        sed -i 's/^blahst_deps=0/blahst_deps=1/' $HOME/.local/bin/blahst.cfg
        echo "Skipping future dependency checks confirmed! To undo, manually set blahst_deps=0 in blahst.cfg."
    fi
    }
    desknote "Dependencies installed" "\nLooks like you have all dependencies installed.\n \
    This check will not run if you set (or have set) blahst_deps=1 in blahst.cfg.\n \
    \nYou can run $0 --help from the Terminal to learn about its options..."
    #---END CHECK DEPENDENCIES. This function will not be called if blahst_deps set to 1 after successful 1st run-------------
}
