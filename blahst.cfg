#X11 or Wayland (2nd line may catch edge cases):
wm="$XDG_SESSION_TYPE"
#wm="${XDG_SESSION_TYPE:-$(loginctl show-session $(loginctl | grep $(whoami) | awk '{print $1}') -p Type --value)}"

#---USER CONFIGURATION BLOCK----------------------------------------------------------------------
#Please, adjust the variables here to suit your environment:
# Store temp files in memory for speed and to reduce SSD/HDD "grinding":
TEMPD='/dev/shm'
# Hardcoded temp wav file to store the speech audio and get overwritten every time (in RAM):
ramf="$TEMPD/wfile"
#deleteramf=0
#Set the number of processing threads for whisper.cpp inference (adjust for your case):
NTHR=8
#It seems that the optimum number of transcribe threads should equal half CPU processing cores:
#NTHR=$(( $(getconf _NPROCESSORS_ONLN) / 2 ))
# Use clipboard to paste from unless PRIMESEL set to 1 to use PRIMARY selection (middle mouse button to paste): 
PRIMESEL=0
# Try to paste automatically on completed transcription, reqires xdotool and such:
AUTOPASTE=1
# Set the next to 1 to default to chat (not sigle shot) mode with the AI assistant of wsiAI:
CHATMODE=0
#Provide hardcoded whisper.cpp hostname and port. To be used when invoked with -n option from CLI:
#(Can be overwritten when command line argument IP:PORT is supplied instead of '-n')
WHOST="127.0.0.1"
WPORT="58080"             
#Provide hardcoded llama.cpp hostname and port.
LHOST="127.0.0.1"
LPORT="58090"             
#The superdirectory where AI models (Whisper, LLMS, TTS etc.) are stored:
AI="$HOME/AI/Models"
# Default whisper.cpp model file for local ASR inference (base.en provides excellent WER for English only)
#WMODEL="$TEMPD/ggml-base.en.bin"
WMODEL=${WHISPER_DMODEL:-"$AI/whisper/ggml-base.en.bin"}
# Uncomment to use available whisperfile (WITH BUILT-IN MODEL) instead of standalone whisper.cpp or whisper.cpp server
#WHISPERFILE="whisper-tiny.en.llamafile" #available from https://huggingface.co/Mozilla/whisperfile/tree/main
#Use the following large language model with llama.cpp for the interactive assistant/translator etc.:
LLMODEL="$AI/gemma-3-4b-it-Q6_K_L.gguf" #avalable from https://huggingface.co/bartowski/gemma3-4b-it-GGUF/tree/main   
#Use the following LIGHTWEIGHT large language model with llama.cpp for a simpler interactive assistant:
LIGHTLMODEL="$AI/gemma-3-1b-it-Q6_K_L.gguf" # available from https://huggingface.co/google/gemma3-1b-it-GGUF
#LIGHTLMODEL="$AI/granite-3.0-2b-instruct-Q6_K_L.gguf"
#Use the following heavy hitter for tasks that require more advanced reasoning (slower):
HEAVYMODEL="$AI/gemma-3-12b-it-q4_0_s.gguf" # available from https://huggingface.co/bartowski/gemma3-12b-it-GGUF
#For coding tasks, invoked with "Programmer...":
CODEMODEL="$AI/Qwen2.5-Coder-14B-Instruct-Q5_K_L.gguf"
#Default llama.cpp executable:
llamf="llam" #This is an existing systemwide symbolic link to llama-cli from lllama.cpp
#Uncomment the following to use a LLM from a llammafile instead of standalone llamma.cpp:
#LLAMAFILE="$AI/gemma-2-2b-it.Q6_K.llamafile"
#LLAMAFILE="llamafile-0.8.16" #available from https://github.com/Mozilla-Ocho/llamafile/releases/tag/0.8.16
#LLAMAFILE="llamafile" # on my system a symbolic link to the current version of llamafile
#Use the following text-to-speech Piper model for human-like audio response in English:
TTSMODEL="$AI/piper/en_US-lessac-low.onnx"  #available from https://github.com/rhasspy/piper/blob/master/VOICES.md
#The above TTS model has sample rate 16000 (if you change the model, adjust the sample rate below):
rtts="16000" 
#Use the following text-to-speech Piper model for human-like audio response in the language (e.g. chinese) of the LLM translator function:
TRANSMODEL="$AI/piper/zh_CN-huayan-medium.onnx"  #available from https://github.com/rhasspy/piper/blob/master/VOICES.md
#The above model has sample rate 22050 (if you change it, adjust the sample rate below):
rtrans="22050"
#Set next to 1 ONLY if the program has confirmed that all dependenncies are met and you do not want to check in the future:
blahst_deps=0 
#---END USER CONFIG BLOCK------------------------------------------------------------------------

#---HELPER FUNCTIONS--------------------------------------------------------------------------
#Notification code, prefers zenity, then notify-send, which should be available across the
#most distributions, in package libnotify or libnotify-bin:
desknote() {
    local title="$1"
    local message="$2"

    if command -v zenity &> /dev/null; then
        zenity --info --text="${title}.\n$message"
    elif command -v notify-send &> /dev/null; then
        notify-send "$title" "$message"
    elif command -v kdialog &> /dev/null; then
        kdialog --passivepopup "$message" 5
    else
        echo "Notification Message: $message" >&2
        echo "Please install zenity or notify-send to use notifications." >&2
        echo "You can install either using your package manager, e.g.:" >&2
        echo "  sudo apt-get install zenity or sudo apt-get install libnotify-bin" >&2
        echo "  sudo yum install zenity or sudo apt-get install libnotify" >&2
        echo "  sudo pacman -S zenity, etc." >&2
    fi
}
#Checks for dependencies:
blahst_depends() {
    #---CHECK DEPENDENCIES.-----------------------------------
    #The install-wsi script should have taken care of this, left here for the manual install.
    command -v curl &>/dev/null || { echo "curl is required. Please, install curl" >&2 ; exit 1 ; }
    #The next is needed if curl requests json output from the whisper.cpp or llama.cpp server;
    command -v jq &>/dev/null || { echo "jq is required. Please, install jq" >&2 ; exit 1 ; }
    command -v sox &>/dev/null || { echo "sox is required. Please, install sox" >&2 ; exit 1 ; }
    [[ -n $WHISPERFILE ]] || command -v transcribe &>/dev/null || { echo -e "Please, install whisper.cpp (see https://github.com/ggerganov/whisper.cpp)\
    \nor download a whisperfile portable executable w model inside (see https://huggingface.co/Mozilla/whisperfile/tree/main)\
    \nand create 'transcribe' in your PATH as a symbolic link to the chosen executable, e.g.\n \
     'ln -s /full/path/to/whisper.cpp/main \$HOME/.local/bin/transcribe'" >&2 ; exit 1 ; }
    command -v llam &>/dev/null || { echo -e "For interaction with an AI, install llama.cpp (see https://github.com/ggerganov/llama.cpp)\
    \nor download a llamafile portable executable (see https://huggingface.co/Mozilla)\
    \nand create in your PATH a symbolic link to the llama-cli executable, e.g.\n \
     'ln -s /full/path/to/llama.cpp/llama-cli \$HOME/.local/bin/llam'" >&2 ; exit 1 ; }
    command -v piper &>/dev/null || { echo -e "Please, install piper text-to-speech (see https://github.com/rhasspy/piper)\
    \nto use the AI assistant and translator features with human-like voice response." >&2 ; exit 1 ; }
    # We will use zenity or notify-send (part of libnotify or libnotify-bin in some distros) for desktop notifications:
    command -v zenity &>/dev/null || command -v notify-send &>/dev/null || { echo "zenity or notify-send needed for error reporting. Please, install zenity or libnotify-bin" >&2 ; exit 1 ; }
    #Now let's check if we are in X11 or Wayland and use the right utility:
    if [[ wm == "wayland" ]]; then
        command -v wl-copy &>/dev/null || { echo "wl-copy is needed for the clipboard. Please, install wl-copy" >&2 ; exit 1 ; } 
        if (( $AUTOPASTE )); then 
           command -v ydotool &>/dev/null || { echo "To have the transcribed text pasted automatically at the cursor position, please install ydotool" >&2 ; exit 1 ; }
        fi
    elif [[ wm == "x11" ]]; then
        command -v xsel &>/dev/null || { echo "We rely on xsel for the clipboard. Please, install xsel." >&2 ; exit 1 ; }
        if (( $AUTOPASTE )); then 
           command -v xdotool &>/dev/null || { echo "To have the transcribed text pasted automatically at the cursor position, please install xdotool" >&2 ; exit 1 ; }
        fi
    fi
    for llv in $WMODEL $LLMODEL $LIGHTLMODEL $HEAVYMODEL $TTSMODEL $TRANSMODEL $CODEMODEL
    do
    [[ -f $llv ]] || { desknote "Model file not found: " "$llv was not found. \nPlease, adjust in USER CONFIGURATION BLOCK." ; exi1 1 ; }
    done 
    [[ -n $LLAMAFILE ]] && { command -v $LLAMAFILE &>/dev/null || { desknote "Not found: " "Executable $LLAMAFILE not found." ; exit 1 ; } }
    [[ -n $WHISPERFILE ]] && { command -v $WHISPERFILE &>/dev/null || { desknote "Not found: " "Executable $WHISPERFILE not found." ; exit 1 } }
    #All PASS...
    command -v zenity &>/dev/null && {
    if zenity --question --text="Dependencies of BlahST are met. Do you want to skip future dependency checks?"; then
        # Update the configuration file to set blahst_deps to 1 (we are sourcing this so hardcoding the filename)
        sed -i 's/^blahst_deps=0/blahst_deps=1/' $HOME/.local/bin/blahst.cfg
        desknote "Skipping future dependency checks confirmed!" "To undo, manually set blahst_deps=0 in blahst.cfg."
    fi
    } || {
    echo -e "\nLooks like you have all dependencies installed.\nTo disable future checks, must set blahst_deps=1 in blahst.cfg.\n"
    read -p "Do you want me to set 'blahst_deps=1' and skip future dependency checks? (y/n) " answer
    if [[ "$answer" == "y" || "$answer" == "Y" ]]; then
        # Update the configuration file to set blahst_deps=1
        sed -i 's/^blahst_deps=0/blahst_deps=1/' $HOME/.local/bin/blahst.cfg
        echo "Skipping future dependency checks confirmed! To undo, manually set blahst_deps=0 in blahst.cfg."
    fi
    }
    desknote "Dependencies installed" "\nLooks like you have all dependencies installed.\n \
    This check will not run if you set (or have set) blahst_deps=1 in blahst.cfg.\n \
    \nYou can run $0 --help from the Terminal to learn about its options..."
    #---END CHECK DEPENDENCIES. This function will not be called if blahst_deps set to 1 after successful 1st run-------------
}
