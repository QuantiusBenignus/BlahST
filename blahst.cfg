#X11 or Wayland (2nd line may catch edge cases):
wm="$XDG_SESSION_TYPE"
#wm="${XDG_SESSION_TYPE:-$(loginctl show-session $(loginctl | grep $(whoami) | awk '{print $1}') -p Type --value)}"

#---USER CONFIGURATION BLOCK----------------------------------------------------------------------
#Please, adjust the variables here to suit your environment
#---------start-section-common
# Store temp files in memory for speed and to reduce SSD/HDD "grinding":
TEMPD='/dev/shm'
# Hardcoded temp wav file to store the speech audio and get overwritten every time (in RAM):
ramf="$TEMPD/wfile"
#deleteramf=0
#Set the number of processing threads for whisper.cpp inference (adjust for your case):
NTHR=8
#It seems that the optimum number of transcribe threads should equal half CPU processing cores:
#NTHR=$(( $(getconf _NPROCESSORS_ONLN) / 2 ))
# Use clipboard to paste from unless PRIMESEL set to 1 to use PRIMARY selection (middle mouse button to paste): 
PRIMESEL=0
# Try to paste automatically on completed transcription, reqires xdotool and such:
AUTOPASTE=0
# Set the next to 1 to default to chat (not sigle shot) mode with the AI assistant of wsiAI:
CHATMODE=0
#Provide hardcoded whisper.cpp hostname and port. To be used when invoked with -n option from CLI:
#(Can be overwritten when command line argument IP:PORT is supplied instead of '-n')
WHOST="127.0.0.1"
WPORT="58080"             
#The superdirectory where AI models (Whisper, LLMS, TTS etc.) are stored:
AI="$HOME/AI/Models"
# Default whisper.cpp model file for local ASR inference (base.en provides excellent WER for English only)
#WMODEL="$TEMPD/ggml-base.en.bin"
WMODEL=${WHISPER_DMODEL:-"$AI/whisper/ggml-base.en.bin"}
# Uncomment to use available whisperfile (WITH BUILT-IN MODEL) instead of standalone whisper.cpp or whisper.cpp server
#WHISPERFILE="whisper-tiny.en.llamafile" #available from https://huggingface.co/Mozilla/whisperfile/tree/main
#--------end-section----wsi--wsiml--blooper

#Provide hardcoded llama.cpp hostname and port.
LHOST="127.0.0.1"
LPORT="58090"             
#Use the following large language model with llama.cpp for the interactive assistant/translator etc.:
LLMODEL="$AI/gemma-3-4b-it-Q6_K_L.gguf" #avalable from https://huggingface.co/bartowski/gemma3-4b-it-GGUF/tree/main   
#Use the following LIGHTWEIGHT large language model with llama.cpp for a simpler interactive assistant:
LIGHTLMODEL="$AI/gemma-3-1b-it-Q6_K_L.gguf" # available from https://huggingface.co/google/gemma3-1b-it-GGUF
#LIGHTLMODEL="$AI/granite-3.0-2b-instruct-Q6_K_L.gguf"
#More potent model with larger VRAM footprint:
HEAVYMODEL="$AI/gemma-3-27b-it-IQ3_XXS.gguf"
#For coding tasks, invoked with "Programmer...":
CODEMODEL="$AI/Qwen2.5-Coder-14B-Instruct-Q5_K_L.gguf"
#Default llama.cpp executable:
llamf="llam" #This is an existing systemwide symbolic link to llama-cli from llama.cpp
#Uncomment the following to use a LLM from a llammafile instead of standalone llamma.cpp:
#LLAMAFILE="$AI/gemma-2-2b-it.Q6_K.llamafile"
#LLAMAFILE="llamafile-0.8.16" #available from https://github.com/Mozilla-Ocho/llamafile/releases/tag/0.8.16
#LLAMAFILE="llamafile" # on my system a symbolic link to the current version of llamafile
#--------end-section----wsiAI

#--------Extras for BLAHSTBOT----------
#Choose a model for the chat bot (Low quantizations chosen on purpose to fit the models in 12 GB VRAM [-ngl 99], for speed):
#BOTMODEL="$AI/gemma-3-12b-it-q4_0_s.gguf" # available from https://huggingface.co/bartowski/gemma3-12b-it-GGUF
#BOTMODEL="$AI/gemma-3-27b-it-IQ3_XXS.gguf"
BOTMODEL="$AI/Mistral-Small-3.2-24B-Instruct-2506-UD-IQ3_XXS.gguf"
#The next is recommended for the Personal Doctor role, assigned automatically in the select_role() function below.
#BOTMODEL="$AI/medgemma-27b-text-it-UD-IQ3_XXS.gguf"
#Choose these common options carefully and optimize for your system hardware. RTFM @ `llama-server --help`
BMOPTIONS="-ngl 99 -fa -c 8192 -nkvo -ctk q8_0 -ctv q8_0 --min-p 0.01 --top-k 64 --top-p 0.95 --no-webui --no-mmap --mlock --log-file $TEMPD/blahstlog "

#System prompts for the chatbot model of choice, add yours here (key, value). The 1st is to shut down the servers from the menu:
typeset -A BMPROMPT=(
["SHUTDOWN"]=''
["Personal Doctor"]='You are MedGemma, a helpful advisor on all things medical.'
["Science  Assistant"]='You are a helpful science assistant who answers briefly and precisely. You use logic and science-based reasoning to provide statements, governed by Physics principles. When you do not have sufficient information, please, say so.'
["Multilingual Assistant"]='You are a helpful assistant who answers briefly and precisely. Ocasionally, I may ask you to speak a specific language. Then, for any language different from English, you will enclose the specific language text in tags with ISO 639-1 codes, for example <fr>Je comprends!</fr>, <es>Una historia de amor</es>. etc. For rapid language switching within a sentence, enclose each non-English segment in tags, for example: This is <zh>你好</zh> and that is <zh>再见</zh>. Do not enclose transliterations such as pinyin and do not enclose english explanations, only the non-English language elements.'
["Language Teacher"]='You are a language teacher. When I ask you to help me with a specific language, you will translate my statements in the desired language, repeating the foreign language statements so I can hear them clearly. Any text that is not in English must be enclosed in tags with ISO 639-1 codes, for example <fr>Je comprends!</fr>, <es>Una historia de amor</es>, <zh>茉莉花茶</zh>, etc. Again, enclose only the non-English text and all the non-English text. Where needed, provide disection of foreign language elements supported by clear English explanations. For example: <zh>乙</zh> (yǐ) in <zh>艺</zh> (yì - art/skill).'
["Language Interpreter"]='You are a language interpreter. When I ask you to interpret to a specific language, you will translate my statements in the desired language, repeating the foreign language statements so I can hear them clearly. Any text that is not in English must be enclosed in tags with ISO 639-1 codes, for example <fr>Je comprends!</fr>, <es>Una historia de amor</es>, <zh>茉莉花茶</zh>, etc. Again, enclose only the non-English text and all the non-English text.'
['French Teacher']='You are highly qualified french language teacher. You will assess my level, interact with me, give me exercises, grade my responses and teach me language concepts as efficiently as possible, in French. Follow proven methodology and make it interesting and informative. All statements in french should be in tags with ISO 639-1 codes, for example <fr>Je comprends!</fr>, <fr>Le Francais est la langue la plus parlee a Geneve</fr>, etc. This is essential since I use a TTS engine to hear proper french pronounciation. So, <fr>N`oiblie pas!</fr>'
["Chinese Translator (streaming)"]='You are a highly accurate and reliable translator between English and Simplified Chinese. Your sole purpose is to translate English text into Simplified Chinese and vice versa, ensuring the translation is as accurate as possible within the given context. Provide only the Chinese translation enclosed in special tags (small circled latin characters) ⓩ and ⓗ, for example: ⓩ茉莉花茶ⓗ , ⓩ 青蓝色的山峰轻抚着蔚蓝的天空。ⓗ, or ⓩ你好ⓗ. When input contains Chinese blocks, translate those blocks to English. When input is in English, translate to simplified Chinese encosed in tags ⓩ and ⓗ. Do not include any explanations, commentary, or additional text within your tagged ⓩ ⓗ translations.'
["Multilingual Assistant (streaming)"]='You are a helpful assistant who answers briefly and precisely. Ocasionally, I may ask you to speak a specific language. Then, for any language different from English, you will enclose the specific language text in special tags (small circled latin characters): - Chinese (zh): ⓩ (U+24E9)...ⓗ (U+24D7)
     - French (fr): ⓕ (U+24D5)...ⓡ (U+24E1)
     - Italian (it): ⓘ (U+24D8)...ⓣ (U+24E3)
     - English (en): No tags needed
     - Spanish (es): ⓔ (U+24D4)...ⓢ (U+24E2)
     - German (de): ⓓ (U+24D3)...ⓔ (U+24D4)), for example: ⓩ茉莉花茶ⓗ, ⓕJe comprends!ⓡ, ⓔUna historia de amor.ⓢ etc. For rapid language switching within a sentence, enclose each non-English segment in these character tags, for example: This is ⓩ你好ⓗ and that is ⓩ再见ⓗ.
     Do not enclose transliterations such as pinyin and do not enclose english explanations, only the non-English language elements: 
             ⓘMi piace questo posto!ⓣ
     Nice place! ⓕJ`aime cet endroit !ⓡ
            ⓩ我喜欢这个地方！ⓗ
    Always verify that every non-English text block is properly enclosed in the appropriate character pair.'
)

#Initial LLM greeting. Modify as desired:
BOTGREETING="I am ready. You can simply speak or select any text with the mouse to append context to your verbal questions. What shall we chat about?"
#Use the following text-to-speech Piper model for human-like audio response in English:
TTSMODEL="$AI/piper/en_US-lessac-low.onnx"  #available from https://github.com/rhasspy/piper/blob/master/VOICES.md
#The above TTS model has sample rate 16000 (if you change the model, adjust the sample rate below):
rtts="16000" 
#Use the following text-to-speech Piper model for human-like audio response in the language (e.g. chinese) of the LLM translator function:
TRANSMODEL="$AI/piper/zh_CN-huayan-medium.onnx"  #available from https://github.com/rhasspy/piper/blob/master/VOICES.md
#The above model has sample rate 22050 (if you change it, adjust the sample rate below):
rtrans="22050"

#Local piper language and model configuration for multilingual speech chat:
PPRMDIR=$AI/piper
#When adding new language models use only ISO 639-1 compliant language codes as keys:
typeset -A TTSM=(
["zh"]="zh_CN-huayan-medium.onnx"
["jo"]="ar_JO-kareem-medium.onnx"
["da"]="da_DK-talesyntese-medium.onnx"
["de"]="de_DE-kerstin-low.onnx"
["el"]="el_GR-rapunzelina-low.onnx"
["gb"]="en_GB-alan-low.onnx"
["us"]="en_US-joe-medium.onnx"
["en"]="en_US-lessac-low.onnx"
["es"]="es_ES-mls_9972-low.onnx"
["fi"]="fi_FI-harri-medium.onnx"
["fr"]="fr_FR-gilles-low.onnx"
["it"]="it_IT-paola-medium.onnx"
["nl"]="nl_NL-mls-medium.onnx"
["pt"]="pt_PT-tugão-medium.onnx"
["ro"]="ro_RO-mihai-medium.onnx"
["ru"]="ru_RU-ruslan-medium.onnx"
["sv"]="sv_SE-nst-medium.onnx"
["tr"]="tr_TR-dfki-medium.onnx"
["ua"]="uk_UA-ukrainian_tts-medium.onnx"
)

typeset -A modrate=(
["zh"]=22050
["jo"]=22050
["da"]=22050
["de"]=16000
["el"]=16000
["gb"]=16000
["us"]=22050
["en"]=16000
["es"]=16000
["fi"]=22050
["fr"]=16000
["it"]=22050
["nl"]=22050
["pt"]=22050
["ro"]=22050
["ru"]=22050
["sv"]=22050
["tr"]=22050
["ua"]=22050
["ua"]=22050
)

#Set next to 1 ONLY if the program has confirmed that all dependenncies are met and you do not want to check in the future:
blahst_deps=0
#Can be done automatically by user input during initial run of blahst_depends(). 
#---END USER CONFIG BLOCK------------------------------------------------------------------------

#---HELPER FUNCTIONS--------------------------------------------------------------------------

#Choose a prompt (role) for the LLM:
select_role () {
    local keys=("${(@k)BMPROMPT}")
    local role=$(zenity --title="BlahstBot Prompt" --height=$((${#keys} * 50)) --width=256 --list --text="Select an LLM Role:" --column="Role" "${keys[@]}")
    BOTPROMPT="${BMPROMPT[$role]}"
    if [[ "$role" == "Personal Doctor" ]]; then BOTMODEL="$AI/medgemma-27b-text-it-UD-IQ3_XXS.gguf"; fi
}

#Notification code, prefers zenity, then notify-send, which should be available across the
#most distributions, in package libnotify or libnotify-bin:
desknote() {
    local title="$1"
    local message="$2"

    if command -v zenity &> /dev/null; then
        zenity --info --text="${title}.\n$message"
    elif command -v notify-send &> /dev/null; then
        notify-send "$title" "$message"
    elif command -v kdialog &> /dev/null; then
        kdialog --passivepopup "$message" 5
    else
        echo "Notification Message: $message" >&2
        echo "Please install zenity or notify-send to use notifications." >&2
        echo "You can install either using your package manager, e.g.:" >&2
        echo "  sudo apt-get install zenity or sudo apt-get install libnotify-bin" >&2
        echo "  sudo yum install zenity or sudo apt-get install libnotify" >&2
        echo "  sudo pacman -S zenity, etc." >&2
    fi
}

#Checks for dependencies:
blahst_depends() {
    #---CHECK DEPENDENCIES.-----------------------------------
    #The install-wsi script should have taken care of this, left here for the manual install.
    command -v curl &>/dev/null || { echo "curl is required. Please, install curl" >&2 ; exit 1 ; }
    #The next is needed if curl requests json output from the whisper.cpp or llama.cpp server;
    command -v jq &>/dev/null || { echo "jq is required. Please, install jq" >&2 ; exit 1 ; }
    command -v sox &>/dev/null || { echo "sox is required. Please, install sox" >&2 ; exit 1 ; }
    [[ -n $WHISPERFILE ]] || command -v transcribe &>/dev/null || { echo -e "Please, install whisper.cpp (see https://github.com/ggerganov/whisper.cpp)\
    \nor download a whisperfile portable executable w model inside (see https://huggingface.co/Mozilla/whisperfile/tree/main)\
    \nand create 'transcribe' in your PATH as a symbolic link to the chosen executable, e.g.\n \
     'ln -s /full/path/to/whisper.cpp/whisper-cli \$HOME/.local/bin/transcribe'" >&2 ; exit 1 ; }
    command -v llam &>/dev/null || { echo -e "For interaction with an AI, install llama.cpp (see https://github.com/ggerganov/llama.cpp)\
    \nor download a llamafile portable executable (see https://huggingface.co/Mozilla)\
    \nand create in your PATH a symbolic link to the llama-cli executable, e.g.\n \
     'ln -s /full/path/to/llama.cpp/llama-cli \$HOME/.local/bin/llam'" >&2 ; exit 1 ; }
    command -v piper &>/dev/null || { echo -e "Please, install piper text-to-speech (see https://github.com/rhasspy/piper)\
    \nto use the AI assistant and translator features with human-like voice response." >&2 ; exit 1 ; }
    # We will use zenity or notify-send (part of libnotify or libnotify-bin in some distros) for desktop notifications:
    command -v zenity &>/dev/null || command -v notify-send &>/dev/null || { echo "zenity or notify-send needed for error reporting. Please, install zenity or libnotify-bin" >&2 ; exit 1 ; }
    #Now let's check if we are in X11 or Wayland and use the right utility:
    if [[ wm == "wayland" ]]; then
        command -v wl-copy &>/dev/null || { echo "wl-copy is needed for the clipboard. Please, install wl-copy" >&2 ; exit 1 ; } 
        if (( $AUTOPASTE )); then 
           command -v ydotool &>/dev/null || { echo "To have the transcribed text pasted automatically at the cursor position, please install ydotool" >&2 ; exit 1 ; }
        fi
    elif [[ wm == "x11" ]]; then
        command -v xsel &>/dev/null || { echo "We rely on xsel for the clipboard. Please, install xsel." >&2 ; exit 1 ; }
        if (( $AUTOPASTE )); then 
           command -v xdotool &>/dev/null || { echo "To have the transcribed text pasted automatically at the cursor position, please install xdotool" >&2 ; exit 1 ; }
        fi
    fi
    for llv in $WMODEL $LLMODEL $LIGHTLMODEL $HEAVYMODEL $TTSMODEL $TRANSMODEL $CODEMODEL $BOTMODEL
    do
    [[ -f $llv ]] || { desknote "Model file not found: " "$llv was not found. \nPlease, adjust in USER CONFIGURATION BLOCK." ; exi1 1 ; }
    done 
    [[ -n $LLAMAFILE ]] && { command -v $LLAMAFILE &>/dev/null || { desknote "Not found: " "Executable $LLAMAFILE not found." ; exit 1 ; } }
    [[ -n $WHISPERFILE ]] && { command -v $WHISPERFILE &>/dev/null || { desknote "Not found: " "Executable $WHISPERFILE not found." ; exit 1 ; } }
    #All PASS...
    command -v zenity &>/dev/null && {
    if zenity --question --text="Dependencies of BlahST are met. Do you want to skip future dependency checks?"; then
        # Update the configuration file to set blahst_deps to 1 (we are sourcing this so hardcoding the filename)
        sed -i 's/^blahst_deps=0/blahst_deps=1/' $HOME/.local/bin/blahst.cfg
        desknote "Skipping future dependency checks confirmed!" "To undo, manually set blahst_deps=0 in blahst.cfg."
    fi
    } || {
    echo -e "\nLooks like you have all dependencies installed.\nTo disable future checks, must set blahst_deps=1 in blahst.cfg.\n"
    read -p "Do you want me to set 'blahst_deps=1' and skip future dependency checks? (y/n) " answer
    if [[ "$answer" == "y" || "$answer" == "Y" ]]; then
        # Update the configuration file to set blahst_deps=1
        sed -i 's/^blahst_deps=0/blahst_deps=1/' $HOME/.local/bin/blahst.cfg
        echo "Skipping future dependency checks confirmed! To undo, manually set blahst_deps=0 in blahst.cfg."
    fi
    }
    desknote "Dependencies installed" "\nLooks like you have all dependencies installed.\n \
    This check will not run if you set (or have set) blahst_deps=1 in blahst.cfg.\n \
    \nYou can run $0 --help from the Terminal to learn about its options..."
    #---END CHECK DEPENDENCIES. This function will not be called if blahst_deps set to 1 after successful 1st run-------------
}
